{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "1) Storing Tasks"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "1-1) Creation of db_restT"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Collects tweets using REST API Search\n",
      "\n",
      "import json, tweepy, string\n",
      "from time import sleep\n",
      "\n",
      "c_key = \"\"\n",
      "c_secret = \"\"\n",
      "\n",
      "wd = \"/media/sf_F_DRIVE/\"\n",
      "\n",
      "auth = tweepy.AppAuthHandler(c_key, c_secret)\n",
      "api = tweepy.API(auth_handler=auth,wait_on_rate_limit=True)\n",
      "\n",
      "\n",
      "class Tweet_Writer():\n",
      "    f = None\n",
      "    \n",
      "    def __init__(self, wd, n=0, outfile_number=0, output_dir=\"tweets_rest\", pad_n=7):\n",
      "        self.wd = wd\n",
      "        self.n = n\n",
      "        self.outfile_number = outfile_number\n",
      "        self.output_dir = output_dir\n",
      "        self.pad_n = pad_n\n",
      "        self.f = open('%s/%s/json/json%s' % (self.wd, self.output_dir, string.zfill(str(self.outfile_number),self.pad_n)),\"wb\")\n",
      "        \n",
      "    def newFile(self):\n",
      "        try:\n",
      "            self.f.close()\n",
      "        except AttributeError:\n",
      "            pass\n",
      "        self.f = open('%s/%s/json/json%s' % (self.wd, self.output_dir, string.zfill(str(self.outfile_number),self.pad_n)),\"wb\")\n",
      "        self.n +=1\n",
      "        \n",
      "    def write(self,tweet):\n",
      "        if (self.n%5001 == 0):\n",
      "            self.outfile_number +=1\n",
      "            print \"Starting new file \", self.outfile_number\n",
      "            self.newFile()\n",
      "        self.f.write(tweet+'\\n')\n",
      "        self.n+=1\n",
      "        \n",
      "\n",
      "\n",
      "query = \"#wimbledon OR #federer\"\n",
      "out = \"no_cursor\"\n",
      "w = Tweet_Writer(wd,output_dir = out)\n",
      "last_id = -1\n",
      "count = 100\n",
      "\n",
      "while True:\n",
      "    new_tweets = api.search(q=query, count=count, max_id=str(last_id - 1))\n",
      "    for tweet in new_tweets:\n",
      "        d = json.dumps(tweet._json)\n",
      "        w.write(d)\n",
      "    last_id = new_tweets[-1].id"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Creates \n",
      "# Remember to handle ValueError on insert_one\n",
      "wd = \"/media/sf_F_DRIVE/no_cursor/\"\n",
      "db_restT = db.db_restT\n",
      "db_restT.create_index(\"user.id\")\n",
      "for i in range(147):\n",
      "    if i%10 == 0:\n",
      "        print i,\n",
      "    with open(wd+\"json/json0000%s\" % str(i+1).zfill(3),\"r\") as f:\n",
      "        for tw in f:\n",
      "            try:\n",
      "                d = json.loads(tw)\n",
      "                db_restT.insert_one(d)\n",
      "            except ValueError:\n",
      "                #ignore improperly formatted lines\n",
      "                pass\n",
      "print \"done\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "1-2) Creation of db_tweets from S3 data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from boto.s3.connection import S3Connection\n",
      "from boto.s3.key import Key\n",
      "\n",
      "aws_key = \"\"\n",
      "secret_key = \"\"\n",
      "bucket = \"mids-w205-mak-a3-tennis\"\n",
      "\n",
      "conn = S3Connection(aws_key, secret_key)\n",
      "bucket = conn.get_bucket(bucket)\n",
      "\n",
      "# Adds tweets to db, index by tweet id\n",
      "db_tweets = db.db_tweets\n",
      "db_tweets.create_index(\"id\")\n",
      "\n",
      "for key in bucket.list():\n",
      "    f = b.get_key(key, validate=False)\n",
      "        for tw in f:\n",
      "            d = json.loads(tw)\n",
      "            db_tweets.insert_one(d)\n",
      "            "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "2) Retrieving and Analyzing"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "2-1) Locating top retweeters"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from bson import SON\n",
      "import pymongo\n",
      "\n",
      "client = pymongo.MongoClient()\n",
      "db = client.testdatabase\n",
      "db_tweets = db.db_tweets\n",
      "\n",
      "pipeline = [{\"$group\": {\"_id\":\"$retweeted_status.id\", \"count\":{\"$sum\":1}}},\n",
      "            {\"$sort\": SON([(\"count\",-1)])}]\n",
      "tweeters = []\n",
      "paired_list = []\n",
      "for doc in db_tweets.aggregate(pipeline):\n",
      "    if doc['_id'] is not None:\n",
      "        tweeter = db_tweets.find_one({\"retweeted_status.id\":doc['_id']})['retweeted_status']['user']\n",
      "        tup = (tweeter['id'],tweeter['name'],tweeter['location'])\n",
      "        t2 = tweeter['followers_count']\n",
      "        if tup not in tweeters and t2<300000:\n",
      "            tweeters.append(tup)\n",
      "            paired_list.append(t2)\n",
      "        if len(tweeters)>=40:\n",
      "            break\n",
      "# Shows user ID, user name, location if listed, and # of followers\n",
      "print zip(tweeters,paired_list)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[((2181831451L, u'Nayanthara Live', u'World Wide Fanclub'), 8787), ((19586117, u'Ryan Love', u'Fermanagh'), 8097), ((542187379, u'judgmental gay', u'tx'), 112091), ((68703042, u'Tennis Photos', u''), 8953), ((50734895, u'Danielle', u''), 525), ((306053167, u'VisitBritain Japan', u''), 23905), ((37831835, u'TodaPasion', u'Argentina'), 267002), ((192812838, u'Bosch DIY & Garden', u'Denham, UK'), 6330), ((2327369137L, u'StellaArtoisUK', u'UK'), 22130), ((571878474, u'Doordarshan National', u'New Delhi'), 84468), ((511217760, u'Mikitype', u'Tokyo, Japan'), 1891), ((3042917002L, u'Carly Rose ', u''), 25054), ((1487653230, u'BBC Get Inspired', u'Salford'), 41317), ((2368371612L, u'ParaDeportes', u''), 1051), ((377282357, u'Deji Kofi Faremi', u'On Twitter'), 27323), ((167070809, u'SI Tennis', u'Tennis courts around the world'), 39420), ((69956111, u'BBC Tennis', u'London, UK.'), 105448), ((19662223, u'Telegraph Pictures', u'London'), 52550), ((2192459389L, u'Godiva Chocolates UK', u'London'), 9340), ((25579394, u'joyce eng', u'ny'), 1762), ((162304163, u'MediaCrooks', u'India'), 97175), ((1561123663, u'Bleacher Report UK', u'Desktop'), 111385), ((307597641, u'Vlade Divac', u'Sacramento, CA'), 52915), ((165012955, u'\\u3058\\u3087\\u30fc\\u3058', u'REISSUE in Harajuku'), 164079), ((25324954, u'ESPNTennis', u''), 254310), ((500676461, u'Rufus The Hawk', u'Airbourne'), 8635), ((451215777, u'Mohandas Menon', u'Mumbai'), 90337), ((406603208, u'Zaid Hamid', u\"Al\\u2011Ha'ir Prison, Saudi Arabia\"), 22102), ((33900164, u'British GQ', u'London, England'), 257226), ((2286378072L, u'PA Sport Pic Desk', u'Nottingham/London'), 1191), ((19292816, u'Betfair', u''), 122725), ((249345746, u'La12tuittera \\u246b ', u'La Bombonera'), 267716), ((18038269, u'The FADER', u'global'), 289699), ((30848360, u'TOI Sports News', u''), 32425), ((410843530, u'Silky Jones', u'Leeds UK'), 41779), ((111556701, u'Daily Mail Celebrity', u''), 136660), ((70547392, u'Heather Watson', u''), 119999), ((2302138069L, u'AnythingCumberbatch', u'United Kingdom'), 10085), ((75311070, u'Gaurav Pandhi', u'New Delhi'), 14873), ((3084417849L, u'MitchumUK', u''), 4224)]\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "2-2) Calculating lexical diversity"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import string, nltk\n",
      "pipeline = [{\"$group\": {\"_id\": \"$user.id\", \"res\":{\"$push\":\"$text\"}}}]\n",
      "lex_db = db.diversity\n",
      "db_restT = db.db_restT\n",
      "translate_table = dict((ord(char), None) for char in string.punctuation)\n",
      "\n",
      "def unique_list(in_list):\n",
      "    #gets the number of unique elements in a list\n",
      "    i=0\n",
      "    rack = []\n",
      "    for element in in_list:\n",
      "        if element not in rack:\n",
      "            rack.append(element)\n",
      "            i=i+1\n",
      "    return i\n",
      "\n",
      "def validate_words(wordList):\n",
      "    returnList = []\n",
      "    for word in wordList:\n",
      "        if nltk.corpus.wordnet.synsets(word):\n",
      "            returnList.append(word)\n",
      "    return returnList\n",
      "\n",
      "x = 0\n",
      "for doc in db_restT.aggregate(pipeline,allowDiskUse = True):\n",
      "    try:\n",
      "        wordList =  validate_words(''.join(doc[\"res\"]).translate(translate_table).lower().split())\n",
      "    except TypeError:\n",
      "        print \"error parsing object\"\n",
      "        print doc\n",
      "        \n",
      "    if len(wordList)>0:\n",
      "        unique_n = unique_list(wordList)\n",
      "        words_n = len(wordList)\n",
      "        diversity = float(unique_n)/words_n\n",
      "    package = {\"id\":doc[\"_id\"],\"unique_words_n\":unique_n,\"all_words_n\":words_n,\"word_text\":wordList, \"lex_diversity\":diversity}\n",
      "    lex_db.insert_one(package)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 60
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "val = []\n",
      "user = []\n",
      "for each in tweeters:\n",
      "    try:\n",
      "        user.append(each[1])\n",
      "        val.append(lex_db.find_one({\"id\":each[0]})['lex_diversity'])\n",
      "    except TypeError:\n",
      "        print \"excluded \"+str(each)\n",
      "        val.append(0)\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "excluded (192812838, u'Bosch DIY & Garden', u'Denham, UK')\n"
       ]
      }
     ],
     "prompt_number": 99
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from matplotlib import pyplot\n",
      "\n",
      "pos = np.arange(len(val))+.5\n",
      "pyplot.barh(pos, val, align='center')\n",
      "pyplot.title('Top retweeters lexical diversity')\n",
      "pyplot.yticks(pos, user)\n",
      "pyplot.ylabel('User Name')\n",
      "\n",
      "pyplot.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 103
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "2-3) Get dropped followers"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sqlite3, pymongo, json\n",
      "conn = sqlite3.connect(':memory:')\n",
      "c = conn.cursor()\n",
      "client = pymongo.MongoClient()\n",
      "db = client.testdatabase\n",
      "coll = db.test_collection\n",
      "f1 = db.week0_followers\n",
      "f2 = db.week1_followers\n",
      "\n",
      "losers = {}\n",
      "\n",
      "\n",
      "for doc in f1.find():\n",
      "    try:\n",
      "        c.execute(\"DROP TABLE f1\")\n",
      "        c.execute(\"DROP TABLE f2\")\n",
      "    except:\n",
      "        pass\n",
      "    c.execute(\"CREATE TABLE f1 (left_table text)\")\n",
      "    c.execute(\"CREATE TABLE f2 (right_table text)\")\n",
      "    output = []\n",
      "\n",
      "    point = doc[\"id\"]\n",
      "    for value in doc[\"value\"]:\n",
      "        c.execute(\"INSERT INTO f1 VALUES (?)\",(str(value),))\n",
      "    doc2 = f2.find_one({\"id\":point})\n",
      "    for value in doc2[\"value\"]:\n",
      "        c.execute(\"INSERT INTO f2 VALUES (?)\",(str(value),))\n",
      "      \n",
      "    c.execute(\"SELECT left_table FROM (SELECT * FROM f1 LEFT OUTER JOIN f2 ON left_table = right_table) WHERE right_table IS NULL\")\n",
      "    # add the actual list into a dictionary for easy reference\n",
      "    with open(\"2-3.dump\",\"a\") as f:\n",
      "        res = c.fetchall()\n",
      "        output.append((point,res))\n",
      "        f.write(json.dumps({point:res})+\"\\n\")\n",
      "    # summarizes the number of followers lost by user\n",
      "    id = db[\"corpus\"].find_one({\"retweeted_status.user.id\":long(point)})\n",
      "    name = id['retweeted_status']['user']['name']\n",
      "    print \"%s lost %i followers within a week\" % (name, len(res))\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Nayanthara Live lost 15 followers within a week\n",
        "Ryan Love lost 6 followers within a week"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Tennis Photos lost 32 followers within a week"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Danielle lost 11 followers within a week"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "VisitBritain Japan lost 67 followers within a week"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Bosch DIY & Garden lost 116 followers within a week"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "StellaArtoisUK lost 57 followers within a week"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Mikitype lost 26 followers within a week"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Carly Rose  lost 194 followers within a week"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "ParaDeportes lost 1 followers within a week"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Godiva Chocolates UK lost 48 followers within a week"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "joyce eng lost 6 followers within a week"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "PA Sport Pic Desk lost 7 followers within a week"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "judgmental gay lost 199 followers within a week"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "TodaPasion lost 265 followers within a week"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Doordarshan National lost 91 followers within a week"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "BBC Get Inspired lost 108 followers within a week"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Deji Kofi Faremi lost 16 followers within a week"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "BBC Tennis lost 321 followers within a week"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "SI Tennis lost 58 followers within a week"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Telegraph Pictures lost 40 followers within a week"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "MediaCrooks lost 141 followers within a week"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Bleacher Report UK lost 250 followers within a week"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Vlade Divac lost 41 followers within a week"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\u3058\u3087\u30fc\u3058 lost 304 followers within a week"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "ESPNTennis lost 235 followers within a week"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Rufus The Hawk lost 21 followers within a week"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Mohandas Menon lost 56 followers within a week"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Zaid Hamid lost 55 followers within a week"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "British GQ lost 297 followers within a week"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "TOI Sports News lost 21 followers within a week"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Silky Jones lost 55 followers within a week"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "AnythingCumberbatch lost 52 followers within a week"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Gaurav Pandhi lost 21 followers within a week"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "MitchumUK lost 68 followers within a week"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Betfair lost 117 followers within a week"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "The FADER lost 486 followers within a week"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "La12tuittera \u246b  lost 417 followers within a week"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Daily Mail Celebrity lost 200 followers within a week"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Heather Watson lost 255 followers within a week"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 48
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "2-4"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "3) Storing and Retrieving"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "3-1) Backing up"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from boto.s3.connection import S3Connection\n",
      "from boto.s3.key import Key\n",
      "import tempfile, pymongo, os, sys, json\n",
      "\n",
      "\n",
      "bucket = \"mids-w205-mak-a3-db-backups\"\n",
      "aws_key = \"\"\n",
      "secret_key = \"\" \n",
      "# Choose mode, \"backup\" or \"restore\"\n",
      "mode = \"backup\" \n",
      "\n",
      "\n",
      "def save_database(database, collection, bucket, access_key, secret_key):\n",
      "    #write to a temporary object first to minimize transfer times\n",
      "    f = tempfile.TemporaryFile()\n",
      "    try:\n",
      "        i = 0\n",
      "        for records in database[collection].find():\n",
      "            # keep track of progress, print file pointer every 10,000 records\n",
      "            if i % 10000 ==0:\n",
      "                print i,\n",
      "            i+=1\n",
      "            f.writelines(str(records)+'\\n')\n",
      "    finally:\n",
      "        conn = S3Connection(access_key, secret_key)\n",
      "        bucket = conn.get_bucket(bucket ,validate = True)\n",
      "        k = Key(bucket)\n",
      "        k.key = collection\n",
      "        k.set_contents_from_file(f,rewind=True)\n",
      "        f.close()\n",
      "\n",
      "        \n",
      "def restore_database(database, collection, bucket, access_key, secret_key):\n",
      "    conn = S3Connection(access_key, secret_key)\n",
      "    bucket = conn.get_bucket(bucket)\n",
      "    f = b.get_key(collection, validate=False)\n",
      "    for line in f:\n",
      "        db[collection].insert_one(json.loads(line))\n",
      "    \n",
      "\n",
      "conn = S3Connection(aws_key, secret_key)\n",
      "bucket = conn.create_bucket(bucket)\n",
      "\n",
      "client = pymongo.MongoClient()\n",
      "db = client.testdatabase\n",
      "\n",
      "if mode == \"backup\":\n",
      "    print \"Creating backup\"\n",
      "    save_coll = \"corpus\"\n",
      "    save_database(db,save_coll,bucket,aws_key,secret_key)\n",
      "    print \"%s saved to S3\" % save_coll\n",
      "    \n",
      "    save_coll = \"db_restT\"\n",
      "    save_database(db,save_coll,bucket,aws_key,secret_key)\n",
      "    print \"%s saved to S3\" % save_coll\n",
      "    \n",
      "elif mode == \"restore\":\n",
      "    print \"Restoring from backup\"\n",
      "    restore_coll = \"corpus\"\n",
      "    db[restore_coll].drop\n",
      "    restore_database(db,restore_coll,bucket,aws_key,secret_key)\n",
      "    print \"%s restored from S3\" % restore_coll\n",
      "    \n",
      "    restore_coll = \"db_restT\"\n",
      "    db[restore_coll].drop\n",
      "    restore_database(db,restore_coll,bucket,aws_key,secret_key)\n",
      "    print \"%s restored from S3\" % restore_coll"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}